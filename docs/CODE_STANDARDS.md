# Code Standards and Style Guidelines\n\nThis document establishes comprehensive coding standards for the AI Knowledge Website project, ensuring consistency, maintainability, and quality across all codebases.\n\n## Table of Contents\n\n1. [General Principles](#general-principles)\n2. [Python Standards](#python-standards)\n3. [TypeScript/JavaScript Standards](#typescriptjavascript-standards)\n4. [SQL Standards](#sql-standards)\n5. [Configuration Standards](#configuration-standards)\n6. [Documentation Standards](#documentation-standards)\n7. [Security Standards](#security-standards)\n8. [Performance Standards](#performance-standards)\n9. [Error Handling Standards](#error-handling-standards)\n10. [Testing Standards](#testing-standards)\n\n## General Principles\n\n### Core Values\n\n1. **Readability First**: Code is read far more often than it's written\n2. **Explicit Over Implicit**: Be explicit about intentions and behavior\n3. **Consistency**: Follow established patterns throughout the codebase\n4. **Simplicity**: Prefer simple solutions over complex ones\n5. **Security**: Security considerations in every design decision\n\n### Universal Guidelines\n\n- **Line Length**: Maximum 100 characters (exceptions for URLs, imports)\n- **Indentation**: Use spaces, not tabs (language-specific sizes)\n- **Encoding**: UTF-8 for all files\n- **Line Endings**: Unix-style (LF) line endings\n- **Trailing Whitespace**: Remove all trailing whitespace\n- **File Naming**: Use descriptive, lowercase names with appropriate separators\n\n## Python Standards\n\n### Style Guide Compliance\n\n**Base Standard**: PEP 8 with modifications\n\n**Tools Configuration**:\n- **Formatter**: Black with line length 100\n- **Import Sorter**: isort with Black profile\n- **Linter**: flake8 with custom configuration\n- **Type Checker**: mypy with strict mode\n\n### Code Structure\n\n**File Organization**:\n\n```python\n#!/usr/bin/env python3\n\"\"\"Module docstring describing the purpose and functionality.\n\nThis module provides content ingestion functionality for the AI Knowledge\nWebsite, including rate-limited web scraping and content validation.\n\nExample:\n    Basic usage of the content ingester:\n    \n    >>> from pipelines.ingest import ContentIngester\n    >>> ingester = ContentIngester(config)\n    >>> content = await ingester.fetch_content(\"https://example.com\")\n\"\"\"\n\n# Standard library imports\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\n# Third-party imports\nimport aiohttp\nimport pydantic\nfrom sqlalchemy import select\n\n# Local imports\nfrom pipelines.config import settings\nfrom pipelines.database.models import ContentItem\nfrom pipelines.exceptions import ContentIngestionError\nfrom pipelines.utils.rate_limiter import RateLimiter\n\n# Module-level constants\nDEFAULT_TIMEOUT = 30\nMAX_RETRIES = 3\nUSER_AGENT = \"AI-Knowledge-Bot/1.0 (+https://ai-knowledge.org/bot)\"\n\n# Module-level logger\nlogger = logging.getLogger(__name__)\n```\n\n### Naming Conventions\n\n```python\n# Good examples\nclass ContentIngester:  # PascalCase for classes\n    \"\"\"Handles content ingestion from external sources.\"\"\"\n    \n    def __init__(self, config: IngestionConfig) -> None:\n        self.rate_limiter = RateLimiter()  # snake_case for variables\n        self._session: Optional[aiohttp.ClientSession] = None  # Private with underscore\n        self.__api_key = config.api_key  # Name mangling for sensitive data\n    \n    async def fetch_content(self, url: str) -> ContentItem:  # snake_case for methods\n        \"\"\"Fetch content from the specified URL.\"\"\"\n        pass\n    \n    @property\n    def is_ready(self) -> bool:  # snake_case for properties\n        \"\"\"Check if the ingester is ready to fetch content.\"\"\"\n        return self._session is not None\n\n# Constants\nMAX_CONTENT_SIZE = 10 * 1024 * 1024  # UPPER_SNAKE_CASE\nDEFAULT_USER_AGENT = \"AI-Knowledge-Bot/1.0\"\n\n# Type aliases for clarity\nContentDict = Dict[str, Any]\nUrlList = List[str]\n```\n\n### Function and Method Design\n\n**Function Signature Standards**:\n\n```python\n# Good: Clear, typed, documented\nasync def process_content_batch(\n    items: List[RawContent],\n    processor_config: ProcessorConfig,\n    batch_size: int = 100,\n    max_retries: int = 3,\n    timeout: float = 30.0\n) -> ProcessingResult:\n    \"\"\"Process a batch of raw content items.\n    \n    Processes content items in batches to optimize performance while\n    maintaining system stability. Failed items are retried according\n    to the retry configuration.\n    \n    Args:\n        items: List of raw content items to process\n        processor_config: Configuration for content processing\n        batch_size: Number of items to process per batch\n        max_retries: Maximum number of retry attempts for failed items\n        timeout: Timeout in seconds for each processing operation\n        \n    Returns:\n        ProcessingResult containing successful and failed items\n        \n    Raises:\n        ProcessingError: If batch processing fails entirely\n        ValidationError: If processor_config is invalid\n        \n    Example:\n        >>> config = ProcessorConfig(quality_threshold=0.8)\n        >>> result = await process_content_batch(raw_items, config)\n        >>> print(f\"Processed {len(result.successful)} items\")\n    \"\"\"\n    if not items:\n        logger.warning(\"Empty content batch provided\")\n        return ProcessingResult(successful=[], failed=[])\n    \n    if batch_size <= 0:\n        raise ValueError(\"Batch size must be positive\")\n    \n    # Implementation here...\n    pass\n\n# Bad: Unclear, untyped, undocumented\ndef process(items, config=None, size=100):\n    # What does this function do?\n    # What types are expected?\n    # What can go wrong?\n    pass\n```\n\n### Error Handling Patterns\n\n```python\n# Good: Specific exceptions with context\nclass ContentIngestionError(Exception):\n    \"\"\"Raised when content ingestion fails.\"\"\"\n    \n    def __init__(self, message: str, url: Optional[str] = None, \n                 status_code: Optional[int] = None) -> None:\n        super().__init__(message)\n        self.url = url\n        self.status_code = status_code\n\nasync def fetch_with_retry(\n    session: aiohttp.ClientSession,\n    url: str,\n    max_retries: int = 3\n) -> str:\n    \"\"\"Fetch URL content with exponential backoff retry.\"\"\"\n    last_exception: Optional[Exception] = None\n    \n    for attempt in range(max_retries + 1):\n        try:\n            async with session.get(url, timeout=30) as response:\n                if response.status == 429:\n                    # Rate limited - wait and retry\n                    wait_time = 2 ** attempt\n                    logger.warning(f\"Rate limited, waiting {wait_time}s before retry\")\n                    await asyncio.sleep(wait_time)\n                    continue\n                \n                response.raise_for_status()\n                return await response.text()\n                \n        except aiohttp.ClientTimeout as e:\n            last_exception = e\n            logger.warning(f\"Timeout fetching {url}, attempt {attempt + 1}/{max_retries + 1}\")\n        except aiohttp.ClientError as e:\n            last_exception = e\n            logger.error(f\"Client error fetching {url}: {e}\")\n            break  # Don't retry client errors\n        \n        if attempt < max_retries:\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n    \n    raise ContentIngestionError(\n        f\"Failed to fetch {url} after {max_retries + 1} attempts\",\n        url=url\n    ) from last_exception\n\n# Bad: Generic error handling\nasync def fetch(url):\n    try:\n        response = requests.get(url)\n        return response.text\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n```\n\n### Data Classes and Models\n\n```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom pydantic import BaseModel, Field, validator\n\n# Good: Using Pydantic for data validation\nclass ContentItem(BaseModel):\n    \"\"\"Represents a content item in the processing pipeline.\"\"\"\n    \n    id: str = Field(..., description=\"Unique identifier for the content\")\n    title: str = Field(..., min_length=1, max_length=500)\n    content: str = Field(..., min_length=10)\n    source_url: str = Field(..., description=\"Original URL of the content\")\n    author: Optional[str] = Field(None, max_length=200)\n    published_date: Optional[datetime] = None\n    tags: List[str] = Field(default_factory=list, max_items=20)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    quality_score: Optional[float] = Field(None, ge=0.0, le=1.0)\n    \n    @validator('source_url')\n    def validate_source_url(cls, v):\n        \"\"\"Validate that source URL is properly formatted.\"\"\"\n        if not v.startswith(('http://', 'https://')):\n            raise ValueError('Source URL must use HTTP or HTTPS protocol')\n        return v\n    \n    @validator('tags')\n    def validate_tags(cls, v):\n        \"\"\"Validate tag format and content.\"\"\"\n        for tag in v:\n            if not tag.strip() or len(tag) > 50:\n                raise ValueError('Tags must be non-empty and <= 50 characters')\n        return [tag.lower().strip() for tag in v]\n    \n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n        json_encoders = {\n            datetime: lambda dt: dt.isoformat()\n        }\n        schema_extra = {\n            \"example\": {\n                \"id\": \"art_20240101_001\",\n                \"title\": \"Introduction to Transformers\",\n                \"content\": \"Transformers are a type of neural network...\",\n                \"source_url\": \"https://example.com/transformers\",\n                \"author\": \"Jane Doe\",\n                \"published_date\": \"2024-01-01T10:00:00Z\",\n                \"tags\": [\"machine-learning\", \"nlp\"],\n                \"quality_score\": 0.92\n            }\n        }\n\n# Alternative: Using dataclasses for simpler cases\n@dataclass(frozen=True)  # Immutable\nclass ProcessingConfig:\n    \"\"\"Configuration for content processing.\"\"\"\n    \n    quality_threshold: float = 0.7\n    max_content_length: int = 100_000\n    enable_cross_linking: bool = True\n    similarity_threshold: float = 0.8\n    processing_timeout: float = 30.0\n    \n    def __post_init__(self) -> None:\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if not 0.0 <= self.quality_threshold <= 1.0:\n            raise ValueError(\"Quality threshold must be between 0.0 and 1.0\")\n        \n        if self.max_content_length <= 0:\n            raise ValueError(\"Max content length must be positive\")\n```\n\n### Logging Standards\n\n```python\nimport logging\nimport structlog\nfrom typing import Any, Dict\n\n# Configure structured logging\nlogging.basicConfig(\n    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n    level=logging.INFO\n)\n\nlogger = structlog.get_logger(__name__)\n\n# Good: Structured, contextual logging\nasync def process_content_item(item: ContentItem) -> ProcessedContent:\n    \"\"\"Process a single content item.\"\"\"\n    logger.info(\n        \"Starting content processing\",\n        content_id=item.id,\n        source=item.source_url,\n        title=item.title[:50]  # Truncate for logs\n    )\n    \n    start_time = time.time()\n    \n    try:\n        # Processing logic here\n        processed = await _perform_processing(item)\n        \n        processing_time = time.time() - start_time\n        logger.info(\n            \"Content processing completed\",\n            content_id=item.id,\n            processing_time=processing_time,\n            quality_score=processed.quality_score\n        )\n        \n        return processed\n        \n    except ProcessingError as e:\n        logger.error(\n            \"Content processing failed\",\n            content_id=item.id,\n            error=str(e),\n            error_type=type(e).__name__\n        )\n        raise\n    except Exception as e:\n        logger.exception(\n            \"Unexpected error during content processing\",\n            content_id=item.id\n        )\n        raise ProcessingError(f\"Unexpected error processing {item.id}\") from e\n\n# Bad: Unstructured, contextless logging\nasync def process_item(item):\n    print(f\"Processing {item.id}\")\n    try:\n        result = do_something(item)\n        print(\"Done\")\n        return result\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise\n```\n\n## TypeScript/JavaScript Standards\n\n### Astro Component Standards\n\n```typescript\n// Good: Well-structured Astro component\n---\n// Component script (runs at build time)\nimport type { ContentCollectionEntry } from 'astro:content';\nimport { formatDate, truncateText } from '../utils';\nimport Badge from './Badge.astro';\n\ninterface Props {\n  /** The article to display */\n  article: ContentCollectionEntry<'articles'>;\n  /** Whether to show the full content or just excerpt */\n  showExcerpt?: boolean;\n  /** Additional CSS classes to apply */\n  class?: string;\n}\n\nconst { \n  article, \n  showExcerpt = false, \n  class: className = '' \n} = Astro.props;\n\nconst { \n  title, \n  description, \n  publishDate, \n  author, \n  tags, \n  category \n} = article.data;\n\nconst formattedDate = formatDate(publishDate);\nconst excerpt = showExcerpt ? truncateText(description, 150) : description;\n---\n\n<article class:list={['article-card', className]}>\n  <header class=\"article-header\">\n    <h2 class=\"article-title\">\n      <a href={`/articles/${article.slug}`}>\n        {title}\n      </a>\n    </h2>\n    \n    <div class=\"article-meta\">\n      <time datetime={publishDate.toISOString()}>\n        {formattedDate}\n      </time>\n      {author && (\n        <span class=\"author\">by {author}</span>\n      )}\n    </div>\n    \n    {category && (\n      <Badge variant=\"category\">{category}</Badge>\n    )}\n  </header>\n  \n  <div class=\"article-content\">\n    <p class=\"description\">{excerpt}</p>\n    \n    {tags.length > 0 && (\n      <ul class=\"tag-list\">\n        {tags.map(tag => (\n          <li key={tag}>\n            <Badge variant=\"tag\">{tag}</Badge>\n          </li>\n        ))}\n      </ul>\n    )}\n  </div>\n  \n  <footer class=\"article-footer\">\n    <a href={`/articles/${article.slug}`} class=\"read-more\">\n      Read more â†’\n    </a>\n  </footer>\n</article>\n\n<style>\n  .article-card {\n    display: flex;\n    flex-direction: column;\n    padding: var(--space-lg);\n    border: 1px solid var(--color-border);\n    border-radius: var(--radius-md);\n    background: var(--color-surface);\n    transition: box-shadow 0.2s ease;\n  }\n  \n  .article-card:hover {\n    box-shadow: var(--shadow-lg);\n  }\n  \n  .article-title {\n    margin: 0 0 var(--space-sm);\n    font-size: var(--font-size-xl);\n    font-weight: var(--font-weight-bold);\n    line-height: var(--line-height-tight);\n  }\n  \n  .article-title a {\n    color: var(--color-text-primary);\n    text-decoration: none;\n  }\n  \n  .article-title a:hover {\n    color: var(--color-primary);\n  }\n  \n  .article-meta {\n    display: flex;\n    gap: var(--space-sm);\n    margin-bottom: var(--space-md);\n    color: var(--color-text-secondary);\n    font-size: var(--font-size-sm);\n  }\n  \n  .tag-list {\n    display: flex;\n    gap: var(--space-xs);\n    flex-wrap: wrap;\n    list-style: none;\n    padding: 0;\n    margin: var(--space-md) 0 0;\n  }\n  \n  .read-more {\n    align-self: flex-start;\n    margin-top: auto;\n    color: var(--color-primary);\n    font-weight: var(--font-weight-medium);\n    text-decoration: none;\n  }\n  \n  .read-more:hover {\n    text-decoration: underline;\n  }\n</style>\n```\n\n### Content Configuration\n\n```typescript\n// src/content/config.ts\nimport { z, defineCollection } from 'astro:content';\n\n// Reusable schema components\nconst baseContentSchema = z.object({\n  title: z.string().min(1, 'Title is required').max(200, 'Title too long'),\n  description: z.string().min(10, 'Description too short').max(500, 'Description too long'),\n  publishDate: z.date(),\n  updatedDate: z.date().optional(),\n  author: z.string().optional(),\n  draft: z.boolean().default(false),\n  featured: z.boolean().default(false)\n});\n\n// Article-specific schema\nconst articleSchema = baseContentSchema.extend({\n  category: z.enum([\n    'machine-learning',\n    'natural-language-processing',\n    'computer-vision',\n    'robotics',\n    'ethics',\n    'research'\n  ]),\n  tags: z.array(z.string()).min(1, 'At least one tag required').max(10, 'Too many tags'),\n  readingTime: z.number().positive().optional(),\n  difficulty: z.enum(['beginner', 'intermediate', 'advanced']).default('intermediate'),\n  relatedArticles: z.array(z.string()).max(5).default([]),\n});\n\n// Glossary entry schema\nconst glossarySchema = z.object({\n  term: z.string().min(1),\n  definition: z.string().min(10),\n  category: z.string(),\n  aliases: z.array(z.string()).default([]),\n  relatedTerms: z.array(z.string()).default([]),\n  difficulty: z.enum(['beginner', 'intermediate', 'advanced']).default('beginner'),\n  source: z.string().url().optional(),\n  examples: z.array(z.string()).default([])\n});\n\n// Collection definitions\nexport const collections = {\n  articles: defineCollection({\n    type: 'content',\n    schema: articleSchema\n  }),\n  glossary: defineCollection({\n    type: 'content', \n    schema: glossarySchema\n  })\n};\n\n// Type exports for use in components\nexport type Article = z.infer<typeof articleSchema>;\nexport type GlossaryEntry = z.infer<typeof glossarySchema>;\n```\n\n### Utility Functions\n\n```typescript\n// src/utils/date.ts\n\n/**\n * Date formatting utilities for consistent date display.\n */\n\n/**\n * Format a date for display in article metadata.\n * \n * @param date - The date to format\n * @param locale - The locale to use (defaults to 'en-US')\n * @param options - Formatting options\n * @returns Formatted date string\n */\nexport function formatDate(\n  date: Date,\n  locale: string = 'en-US',\n  options: Intl.DateTimeFormatOptions = {\n    year: 'numeric',\n    month: 'long', \n    day: 'numeric'\n  }\n): string {\n  return new Intl.DateTimeFormat(locale, options).format(date);\n}\n\n/**\n * Format a date as a relative time string.\n * \n * @param date - The date to format\n * @param baseDate - The date to compare against (defaults to now)\n * @returns Relative time string (e.g., \"2 days ago\")\n */\nexport function formatRelativeTime(\n  date: Date,\n  baseDate: Date = new Date()\n): string {\n  const rtf = new Intl.RelativeTimeFormat('en', { numeric: 'auto' });\n  const diffInSeconds = (date.getTime() - baseDate.getTime()) / 1000;\n  \n  const intervals = {\n    year: 31536000,\n    month: 2628000,\n    week: 604800,\n    day: 86400,\n    hour: 3600,\n    minute: 60\n  };\n  \n  for (const [unit, seconds] of Object.entries(intervals)) {\n    const interval = Math.floor(Math.abs(diffInSeconds) / seconds);\n    if (interval >= 1) {\n      return rtf.format(\n        diffInSeconds < 0 ? -interval : interval,\n        unit as Intl.RelativeTimeFormatUnit\n      );\n    }\n  }\n  \n  return 'just now';\n}\n\n/**\n * Check if a date is within the last N days.\n * \n * @param date - The date to check\n * @param days - Number of days to check within\n * @returns True if date is within the specified number of days\n */\nexport function isWithinDays(date: Date, days: number): boolean {\n  const now = new Date();\n  const diffInDays = (now.getTime() - date.getTime()) / (1000 * 3600 * 24);\n  return diffInDays <= days;\n}\n```\n\n## SQL Standards\n\n### Schema Design\n\n```sql\n-- Good: Well-structured table with proper constraints\nCREATE TABLE content_items (\n    -- Primary key with descriptive name\n    id SERIAL PRIMARY KEY,\n    \n    -- Required fields with NOT NULL constraints\n    title VARCHAR(500) NOT NULL,\n    content TEXT NOT NULL,\n    source_url TEXT NOT NULL,\n    content_hash VARCHAR(64) NOT NULL UNIQUE,\n    \n    -- Optional fields with appropriate defaults\n    author VARCHAR(200),\n    published_date TIMESTAMPTZ,\n    quality_score FLOAT CHECK (quality_score >= 0.0 AND quality_score <= 1.0),\n    \n    -- Status with enum constraint\n    status VARCHAR(20) NOT NULL DEFAULT 'pending'\n        CHECK (status IN ('pending', 'processing', 'published', 'archived', 'failed')),\n    \n    -- Foreign key with descriptive name\n    source_id INTEGER NOT NULL REFERENCES sources(id) ON DELETE RESTRICT,\n    \n    -- Audit fields\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    \n    -- Metadata as JSONB for flexibility\n    metadata JSONB DEFAULT '{}'\n);\n\n-- Indexes for performance\nCREATE INDEX idx_content_items_source_id ON content_items(source_id);\nCREATE INDEX idx_content_items_status ON content_items(status);\nCREATE INDEX idx_content_items_created_at ON content_items(created_at DESC);\nCREATE INDEX idx_content_items_published_date ON content_items(published_date DESC) \n    WHERE published_date IS NOT NULL;\nCREATE INDEX idx_content_items_quality_score ON content_items(quality_score DESC) \n    WHERE quality_score IS NOT NULL;\n\n-- Partial index for active content\nCREATE INDEX idx_content_items_active ON content_items(created_at DESC) \n    WHERE status IN ('published', 'processing');\n\n-- GIN index for JSONB metadata searching\nCREATE INDEX idx_content_items_metadata ON content_items USING GIN(metadata);\n\n-- Update trigger for updated_at\nCREATE OR REPLACE FUNCTION update_updated_at_column()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = NOW();\n    RETURN NEW;\nEND;\n$$ language 'plpgsql';\n\nCREATE TRIGGER update_content_items_updated_at \n    BEFORE UPDATE ON content_items\n    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n```\n\n### Query Standards\n\n```sql\n-- Good: Well-formatted, efficient query\nSELECT \n    ci.id,\n    ci.title,\n    ci.author,\n    ci.published_date,\n    ci.quality_score,\n    s.name AS source_name,\n    s.type AS source_type,\n    -- Aggregate related data\n    COUNT(d.id) AS duplicate_count,\n    -- JSON aggregation for tags\n    COALESCE(\n        JSON_AGG(\n            DISTINCT t.name \n            ORDER BY t.name\n        ) FILTER (WHERE t.name IS NOT NULL), \n        '[]'::json\n    ) AS tags\nFROM content_items ci\nINNER JOIN sources s ON ci.source_id = s.id\nLEFT JOIN duplicates d ON ci.id = d.primary_item_id\nLEFT JOIN content_tags ct ON ci.id = ct.content_id\nLEFT JOIN tags t ON ct.tag_id = t.id\nWHERE \n    ci.status = 'published'\n    AND ci.quality_score >= $1\n    AND ci.published_date >= $2\n    AND (\n        $3 IS NULL \n        OR s.type = ANY($3::text[])\n    )\nGROUP BY \n    ci.id, \n    ci.title, \n    ci.author, \n    ci.published_date, \n    ci.quality_score,\n    s.name, \n    s.type\nHAVING \n    COUNT(d.id) < 5  -- Exclude items with too many duplicates\nORDER BY \n    ci.quality_score DESC,\n    ci.published_date DESC\nLIMIT $4 OFFSET $5;\n\n-- Bad: Poorly formatted, inefficient query\nselect * from content_items ci, sources s where ci.source_id=s.id and ci.status='published' order by ci.created_at;\n```\n\n### Migration Standards\n\n```sql\n-- migrations/004_add_content_quality_scoring.sql\n\n-- Migration: Add content quality scoring\n-- Author: Engineering Team\n-- Date: 2024-01-15\n-- Description: Adds quality scoring functionality to content items\n\n-- ====== UP MIGRATION ======\n\nBEGIN;\n\n-- Add quality_score column\nALTER TABLE content_items \nADD COLUMN quality_score FLOAT;\n\n-- Add check constraint for valid score range\nALTER TABLE content_items \nADD CONSTRAINT ck_content_items_quality_score_range \nCHECK (quality_score IS NULL OR (quality_score >= 0.0 AND quality_score <= 1.0));\n\n-- Create index for quality score queries\nCREATE INDEX idx_content_items_quality_score \nON content_items(quality_score DESC) \nWHERE quality_score IS NOT NULL;\n\n-- Create quality_metrics table for detailed scoring\nCREATE TABLE quality_metrics (\n    id SERIAL PRIMARY KEY,\n    content_id INTEGER NOT NULL REFERENCES content_items(id) ON DELETE CASCADE,\n    metric_name VARCHAR(50) NOT NULL,\n    metric_value FLOAT NOT NULL,\n    calculated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    \n    UNIQUE(content_id, metric_name)\n);\n\nCREATE INDEX idx_quality_metrics_content_id ON quality_metrics(content_id);\nCREATE INDEX idx_quality_metrics_name ON quality_metrics(metric_name);\n\n-- Insert version record\nINSERT INTO schema_migrations (version, description, applied_at) \nVALUES (4, 'Add content quality scoring', NOW());\n\nCOMMIT;\n\n-- ====== DOWN MIGRATION ======\n-- Uncomment to rollback this migration\n\n/*\nBEGIN;\n\n-- Remove quality metrics table\nDROP TABLE IF EXISTS quality_metrics;\n\n-- Remove quality score index\nDROP INDEX IF EXISTS idx_content_items_quality_score;\n\n-- Remove quality score constraint\nALTER TABLE content_items \nDROP CONSTRAINT IF EXISTS ck_content_items_quality_score_range;\n\n-- Remove quality score column\nALTER TABLE content_items \nDROP COLUMN IF EXISTS quality_score;\n\n-- Remove version record\nDELETE FROM schema_migrations WHERE version = 4;\n\nCOMMIT;\n*/\n```\n\n## Configuration Standards\n\n### Environment Configuration\n\n```yaml\n# Good: Well-organized configuration\n# config/production.yaml\n\n# Application settings\napp:\n  name: \"AI Knowledge Website\"\n  version: \"1.2.3\"\n  environment: \"production\"\n  debug: false\n  log_level: \"INFO\"\n\n# Database configuration\ndatabase:\n  host: \"${DATABASE_HOST}\"\n  port: 5432\n  name: \"ai_knowledge_prod\"\n  username: \"${DATABASE_USER}\"\n  password: \"${DATABASE_PASSWORD}\"\n  pool_size: 20\n  max_overflow: 30\n  echo: false\n  ssl_mode: \"require\"\n  connection_timeout: 30\n\n# Redis configuration\nredis:\n  host: \"${REDIS_HOST}\"\n  port: 6379\n  database: 0\n  password: \"${REDIS_PASSWORD}\"\n  max_connections: 100\n  socket_timeout: 30\n  socket_connect_timeout: 30\n\n# Content pipeline settings\npipeline:\n  scraping:\n    request_delay: 1.0\n    max_retries: 3\n    timeout: 30\n    concurrent_requests: 5\n    respect_robots_txt: true\n    user_agent: \"AI-Knowledge-Bot/1.0 (+https://ai-knowledge.org/bot)\"\n    \n  deduplication:\n    simhash_threshold: 3\n    minhash_threshold: 0.85\n    lsh_num_perm: 256\n    content_min_length: 100\n    \n  enrichment:\n    max_summary_length: 500\n    similarity_threshold: 0.7\n    max_related_articles: 5\n    enable_cross_linking: true\n    \n  quality:\n    min_quality_score: 0.7\n    enable_manual_review: true\n    review_threshold: 0.75\n\n# External API configurations\nexternal_apis:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n    model: \"gpt-4\"\n    max_tokens: 2000\n    temperature: 0.3\n    rate_limit: 60  # requests per minute\n    \n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"\n    model: \"claude-3-sonnet-20240229\"\n    max_tokens: 2000\n    rate_limit: 50\n\n# Monitoring and observability\nmonitoring:\n  prometheus:\n    enabled: true\n    port: 8080\n    path: \"/metrics\"\n    \n  logging:\n    format: \"json\"\n    level: \"INFO\"\n    file_path: \"/var/log/ai-knowledge/app.log\"\n    max_file_size: \"100MB\"\n    backup_count: 5\n    \n  sentry:\n    dsn: \"${SENTRY_DSN}\"\n    environment: \"production\"\n    sample_rate: 0.1\n    traces_sample_rate: 0.01\n\n# Security settings\nsecurity:\n  secret_key: \"${SECRET_KEY}\"\n  api_key_header: \"X-API-Key\"\n  rate_limiting:\n    enabled: true\n    default_limit: \"100/hour\"\n    burst_limit: 20\n    \n  cors:\n    allowed_origins:\n      - \"https://ai-knowledge.org\"\n      - \"https://www.ai-knowledge.org\"\n    allowed_methods: [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n    allowed_headers: [\"Content-Type\", \"Authorization\", \"X-API-Key\"]\n    expose_headers: [\"X-RateLimit-Limit\", \"X-RateLimit-Remaining\"]\n    max_age: 86400\n```\n\n### Environment Variables\n\n```bash\n# .env.example - Template for environment variables\n\n# ============================================================================\n# AI Knowledge Website Environment Configuration\n# ============================================================================\n\n# Environment\nENVIRONMENT=development\nDEBUG=true\n\n# Database Configuration\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\nDATABASE_NAME=ai_knowledge_dev\nDATABASE_USER=postgres\nDATABASE_PASSWORD=your_password_here\n\n# Redis Configuration\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\nREDIS_DATABASE=0\n\n# API Keys (required for content enrichment)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=...\n\n# Security\nSECRET_KEY=your_secret_key_here_make_it_long_and_random\nJWT_SECRET_KEY=your_jwt_secret_here\n\n# External Services\nSENTRY_DSN=https://...\nCLOUDFLARE_API_KEY=...\nCLOUDFLARE_ZONE_ID=...\n\n# Pipeline Configuration\nSCRAPING_REQUEST_DELAY=1.0\nSCRAPING_CONCURRENT_REQUESTS=5\nDUPLICATE_DETECTION_THRESHOLD=0.85\nQUALITY_SCORE_THRESHOLD=0.7\n\n# Monitoring\nPROMETHEUS_ENABLED=true\nPROMETHEUS_PORT=8080\nLOG_LEVEL=INFO\nLOG_FORMAT=json\n\n# Development settings\nRELOAD=true\nHOT_RELOAD=true\nPROFILING_ENABLED=false\n```\n\n## Documentation Standards\n\n### Code Documentation\n\n```python\n# Good: Comprehensive module documentation\n\"\"\"Content duplicate detection module.\n\nThis module provides functionality for detecting duplicate content using\nvarious algorithms including SimHash and MinHash LSH. It's designed to\nhandle large-scale content deduplication with configurable similarity\nthresholds and performance optimizations.\n\nThe module supports:\n- SimHash-based duplicate detection for near-identical content\n- MinHash LSH for finding similar content at scale  \n- Configurable similarity thresholds\n- Batch processing for performance\n- Detailed similarity metrics\n\nTypical usage:\n    >>> from pipelines.dedup import DuplicateDetector\n    >>> detector = DuplicateDetector(algorithm='simhash', threshold=3)\n    >>> is_duplicate = detector.check_duplicate(content1, content2)\n    >>> print(f\"Content is duplicate: {is_duplicate}\")\n\nAttributes:\n    DEFAULT_SIMHASH_THRESHOLD (int): Default threshold for SimHash comparison\n    DEFAULT_MINHASH_THRESHOLD (float): Default threshold for MinHash comparison\n    SUPPORTED_ALGORITHMS (List[str]): List of supported detection algorithms\n\nNote:\n    This module requires the datasketch library for LSH functionality.\n    Install with: pip install datasketch\n    \nSee Also:\n    pipelines.normalize: Content normalization before duplicate detection\n    pipelines.database.models: Database models for storing duplicate relationships\n    \nReferences:\n    - SimHash: Charikar, Moses S. \"Similarity estimation techniques from \n      rounding algorithms.\" Proceedings of the thiry-fourth annual ACM \n      symposium on Theory of computing. 2002.\n    - MinHash LSH: Broder, Andrei Z. \"On the resemblance and containment of \n      documents.\" Proceedings. Compression and Complexity of SEQUENCES 1997.\n\"\"\"\n\nimport hashlib\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Set, Tuple, Union\n\nfrom datasketch import MinHashLSH, MinHash\nfrom sqlalchemy.orm import Session\n\nfrom pipelines.config import settings\nfrom pipelines.database.models import ContentItem, Duplicate\nfrom pipelines.exceptions import DuplicateDetectionError\nfrom pipelines.utils.text import normalize_text, tokenize\n\n# Module constants\nDEFAULT_SIMHASH_THRESHOLD = 3\nDEFAULT_MINHASH_THRESHOLD = 0.8\nSUPPORTED_ALGORITHMS = ['simhash', 'minhash', 'hybrid']\n\n# Module logger\nlogger = logging.getLogger(__name__)\n```\n\n### API Documentation\n\n```python\nfrom fastapi import FastAPI, HTTPException, Depends, status\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass DuplicateDetectionRequest(BaseModel):\n    \"\"\"Request model for duplicate detection API.\n    \n    Attributes:\n        content: The content text to check for duplicates\n        algorithm: Detection algorithm to use (simhash, minhash, hybrid)\n        threshold: Similarity threshold (algorithm-specific)\n        exclude_ids: Content IDs to exclude from duplicate check\n    \"\"\"\n    \n    content: str = Field(\n        ...,\n        min_length=10,\n        max_length=100000,\n        description=\"Content text to check for duplicates\",\n        example=\"This is a sample article about machine learning...\"\n    )\n    algorithm: str = Field(\n        default=\"simhash\",\n        description=\"Detection algorithm to use\",\n        regex=\"^(simhash|minhash|hybrid)$\"\n    )\n    threshold: Optional[float] = Field(\n        default=None,\n        ge=0.0,\n        le=1.0,\n        description=\"Similarity threshold (0.0 to 1.0)\"\n    )\n    exclude_ids: List[str] = Field(\n        default_factory=list,\n        description=\"Content IDs to exclude from duplicate check\",\n        max_items=100\n    )\n\n@app.post(\n    \"/api/v1/detect-duplicates\",\n    response_model=DuplicateDetectionResponse,\n    status_code=status.HTTP_200_OK,\n    summary=\"Detect duplicate content\",\n    description=\"\"\"\n    Detect if the provided content is a duplicate of existing content.\n    \n    This endpoint uses advanced similarity detection algorithms to identify\n    potential duplicate content in the database. It supports multiple\n    algorithms with configurable thresholds.\n    \n    **Algorithms:**\n    - `simhash`: Fast bit-based similarity detection, good for near-identical content\n    - `minhash`: Jaccard similarity estimation, good for finding similar content\n    - `hybrid`: Combines both algorithms for comprehensive detection\n    \n    **Thresholds:**\n    - SimHash: Integer (0-64), lower values are more strict\n    - MinHash: Float (0.0-1.0), higher values are more strict\n    - Hybrid: Uses algorithm-specific defaults if not specified\n    \n    **Rate Limiting:** 100 requests per hour per API key\n    \n    **Performance:** Typical response time <200ms for content up to 10KB\n    \"\"\",\n    responses={\n        200: {\n            \"description\": \"Duplicate detection completed successfully\",\n            \"content\": {\n                \"application/json\": {\n                    \"examples\": {\n                        \"duplicates_found\": {\n                            \"summary\": \"Duplicates found\",\n                            \"value\": {\n                                \"is_duplicate\": True,\n                                \"similarity_score\": 0.95,\n                                \"algorithm_used\": \"simhash\",\n                                \"matches\": [\n                                    {\n                                        \"content_id\": \"art_20240101_001\",\n                                        \"similarity\": 0.95,\n                                        \"title\": \"Similar Article Title\"\n                                    }\n                                ],\n                                \"processing_time_ms\": 45\n                            }\n                        },\n                        \"no_duplicates\": {\n                            \"summary\": \"No duplicates found\", \n                            \"value\": {\n                                \"is_duplicate\": False,\n                                \"similarity_score\": 0.12,\n                                \"algorithm_used\": \"simhash\",\n                                \"matches\": [],\n                                \"processing_time_ms\": 32\n                            }\n                        }\n                    }\n                }\n            }\n        },\n        400: {\n            \"description\": \"Invalid request parameters\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\n                        \"detail\": \"Content too short for reliable duplicate detection\"\n                    }\n                }\n            }\n        },\n        429: {\n            \"description\": \"Rate limit exceeded\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\n                        \"detail\": \"Rate limit exceeded. Try again in 3600 seconds.\"\n                    }\n                }\n            }\n        }\n    },\n    tags=[\"Content Analysis\"]\n)\nasync def detect_duplicates(\n    request: DuplicateDetectionRequest,\n    db: Session = Depends(get_db_session),\n    current_user: User = Depends(get_current_user)\n) -> DuplicateDetectionResponse:\n    \"\"\"Detect duplicate content using specified algorithm.\n    \n    Args:\n        request: Duplicate detection request parameters\n        db: Database session dependency\n        current_user: Authenticated user dependency\n        \n    Returns:\n        DuplicateDetectionResponse with detection results\n        \n    Raises:\n        HTTPException: If detection fails or parameters are invalid\n    \"\"\"\n    # Implementation here...\n    pass\n```\n\n---\n\n**Enforcement**: These standards are enforced through:\n- Pre-commit hooks for automatic formatting\n- CI/CD pipeline quality gates \n- Code review requirements\n- Automated testing requirements\n- Documentation coverage checks\n\n**Updates**: This document is updated as the project evolves. See the git history for changes.\n\n**Last Updated**: January 2024  \n**Version**: 1.0.0"}]