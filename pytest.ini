[tool:pytest]
# Pytest configuration for the AI Knowledge Website project

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test* *Test
python_functions = test_*

# Minimum Python version
minversion = 3.10

# Required plugins
required_plugins = 
    pytest-asyncio
    pytest-mock
    pytest-cov
    pytest-xdist
    pytest-benchmark
    pytest-timeout

# Default options
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=pipelines
    --cov-report=term-missing
    --cov-report=html:coverage/html-report
    --cov-report=xml:coverage/coverage.xml
    --cov-report=lcov:coverage/lcov.info
    --cov-branch
    --cov-fail-under=95
    --timeout=300
    --durations=10

# Test markers
markers =
    unit: Unit tests
    integration: Integration tests
    performance: Performance tests
    security: Security tests
    database: Database tests
    slow: Slow running tests (use --run-slow-tests to include)
    network: Tests requiring network access
    ai_api: Tests requiring AI API access
    
    # Component-specific markers
    ingest: RSS ingestion tests
    normalize: Content normalization tests
    dedup: Duplicate detection tests
    enrich: Content enrichment tests
    publish: Publishing tests
    
    # Environment markers
    local: Local environment only
    ci: CI environment tests
    staging: Staging environment tests
    production: Production environment tests (manual only)

# Coverage configuration
[coverage:run]
source = pipelines
branch = true
omit = 
    tests/*
    */__pycache__/*
    */migrations/*
    */venv/*
    */env/*
    .venv/*
    setup.py
    conftest.py
    */.pytest_cache/*

[coverage:report]
# Fail if coverage is below threshold
fail_under = 95

# Show missing lines
show_missing = true

# Skip covered files in report
skip_covered = false

# Skip empty files
skip_empty = false

# Precision for percentages
precision = 2

# Exclude lines from coverage
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[coverage:html]
directory = coverage/html-report
title = AI Knowledge Website Pipeline Coverage
show_contexts = true

[coverage:xml]
output = coverage/coverage.xml

# AsyncIO configuration
asyncio_mode = auto

# Timeout configuration
timeout = 300
timeout_method = thread

# Warnings configuration
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    error::UserWarning
    
# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Benchmark configuration (for pytest-benchmark)
benchmark_min_time = 0.000005
benchmark_max_time = 1.0
benchmark_min_rounds = 5
benchmark_timer = perf_counter
benchmark_disable_gc = false
benchmark_warmup = false
benchmark_warmup_iterations = 100000

# Parallel execution configuration
dist = worksteal
tx = popen//python=sys.executable

# Custom command line options
addopts_local = 
    --run-slow-tests
    --benchmark-skip

addopts_ci = 
    --numprocesses=auto
    --benchmark-skip
    --timeout=600
    
# Test collection configuration
collect_ignore = [
    "setup.py",
    "build",
    "dist",
    ".tox",
    ".pytest_cache"
]

# xfail strict mode
xfail_strict = true

# Console output width
console_output_style = progress

# Doctest configuration
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL ELLIPSIS

# JUnit XML output for CI
junit_suite_name = AI_Knowledge_Pipeline_Tests
junit_logging = system-out
junit_log_passing_tests = false