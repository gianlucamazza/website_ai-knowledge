name: CI - Pull Request Validation

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened]
  push:
    branches: [develop]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: '18.x'
  PYTHON_VERSION: '3.11'
  DEPENDENCY_TIER: 'ci'  # Use lightweight CI dependencies

jobs:
  # Job 1: Code Quality and Linting (Optimized)
  quality-checks:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    timeout-minutes: 8  # Reduced from 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js with optimized caching
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: apps/site/package-lock.json

      - name: Setup Python with optimized caching
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: |
            pipelines/requirements-ci.txt
            pipelines/requirements.txt

      - name: Install dependencies with timeout and retry
        run: |
          chmod +x scripts/install-dependencies.sh
          scripts/install-dependencies.sh --tier ci --timeout 300

      - name: Validate package.json and package-lock.json
        run: cd apps/site && npm audit --audit-level=high

      - name: Run Prettier format check
        run: cd apps/site && npx prettier --check "src/**/*.{ts,astro,md,json}"

      - name: Run ESLint (if configured)
        run: cd apps/site && npm run lint:eslint || echo "ESLint not configured, skipping..."
        continue-on-error: true

      - name: Run Python code formatting check
        run: cd pipelines && python -m black --check --diff .

      - name: Run Python import sorting check
        run: cd pipelines && python -m isort --check-only --diff .

      - name: Run Python type checking
        run: cd pipelines && python -m mypy . --ignore-missing-imports

  # Job 2: Build Validation (Optimized)
  build-validation:
    name: Build Validation
    runs-on: ubuntu-latest
    timeout-minutes: 12  # Reduced from 15
    needs: quality-checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js with optimized caching
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: apps/site/package-lock.json

      - name: Install dependencies with retry logic
        working-directory: apps/site
        run: |
          for i in {1..3}; do
            npm ci && break || (echo "Attempt $i failed, retrying in 5s..." && sleep 5)
          done

      - name: Run TypeScript compilation check
        run: cd apps/site && npx tsc --noEmit

      - name: Run Astro check
        run: cd apps/site && npm run astro check

      - name: Build Astro site
        run: cd apps/site && npm run build

      - name: Validate build output
        run: |
          if [ ! -d "apps/site/dist" ]; then
            echo "Build failed: dist directory not found"
            exit 1
          fi
          
          if [ ! -f "apps/site/dist/index.html" ]; then
            echo "Build failed: index.html not found"
            exit 1
          fi
          
          echo "Build validation passed"

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts-${{ github.run_id }}
          path: apps/site/dist/
          retention-days: 1

  # Job 3: Content Validation (Optimized)
  content-validation:
    name: Content Validation
    runs-on: ubuntu-latest
    timeout-minutes: 8  # Reduced from 10
    needs: quality-checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js with caching
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: apps/site/package-lock.json

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          cd apps/site && npm ci
          pip install markdownlint-cli2 markdown-link-check

      - name: Run markdown linting
        run: cd apps/site && npm run lint

      - name: Validate content schemas
        run: cd apps/site && npm run astro check

      - name: Run custom content validation
        run: |
          python scripts/validate_content.py --content-dir apps/site/src/content || echo "Content validation script not found, skipping..."
        continue-on-error: true

      - name: Check for broken links (with timeout)
        run: |
          timeout 300 find apps/site/src/content -name "*.md" -exec markdown-link-check {} \; || echo "Link checking completed with warnings"
        continue-on-error: true

  # Job 4: Python Pipeline Testing (Optimized)
  pipeline-tests:
    name: Python Pipeline Tests
    runs-on: ubuntu-latest
    timeout-minutes: 12  # Reduced from 15
    needs: quality-checks
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_ai_knowledge
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python with caching
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: pipelines/requirements-ci.txt

      - name: Install dependencies with timeout handling
        run: |
          chmod +x scripts/install-dependencies.sh
          scripts/install-dependencies.sh --tier ci --timeout 600

      - name: Set up test environment
        env:
          DATABASE_URL: postgresql://postgres:test@localhost:5432/test_ai_knowledge
        run: |
          cd pipelines
          python -c "
          import asyncio
          from database.connection import create_tables
          asyncio.run(create_tables())
          " || echo "Database setup skipped - no create_tables function found"

      - name: Run Python unit tests with optimized settings
        env:
          DATABASE_URL: postgresql://postgres:test@localhost:5432/test_ai_knowledge
          PYTHONPATH: ${{ github.workspace }}
          TEST_ENVIRONMENT: ci
          TEST_MOCK_AI_APIS: true
        run: |
          cd ${{ github.workspace }}
          python -m pytest tests/unit/ -v \
            --maxfail=5 \
            --tb=short \
            --cov=pipelines \
            --cov-report=xml:coverage/coverage.xml \
            --cov-report=term-missing \
            --durations=10 \
            --timeout=60

      - name: Generate analysis summary (fallback mechanism)
        if: always()
        run: |
          chmod +x scripts/generate-analysis-summary.py
          python scripts/generate-analysis-summary.py --workspace . --output analysis_summary.md --verbose

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: coverage/coverage.xml
          flags: python-pipeline
          name: pipeline-coverage
        continue-on-error: true

      - name: Upload analysis summary artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: analysis-summary-${{ github.run_id }}
          path: |
            analysis_summary.md
            dependency-installation-report.md
          retention-days: 7

  # Job 5: Frontend Testing (Optimized with Dynamic Ports)
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: build-validation
    
    strategy:
      matrix:
        node-version: [18, 20]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: apps/site/package-lock.json

      - name: Allocate dynamic ports to avoid conflicts
        id: port-allocation
        run: |
          chmod +x scripts/port-allocator.js
          node scripts/port-allocator.js allocate node-${{ matrix.node-version }}-${{ github.run_id }}
          echo "Port allocation completed for job: node-${{ matrix.node-version }}-${{ github.run_id }}"

      - name: Install dependencies
        working-directory: apps/site
        run: npm ci

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts-${{ github.run_id }}
          path: apps/site/dist/

      - name: Type check
        working-directory: apps/site
        run: npm run type-check

      - name: Run unit tests with coverage
        working-directory: apps/site
        env:
          NODE_ENV: test
        run: |
          npm run test:unit:coverage -- \
            --reporter=junit \
            --reporter=default \
            --outputFile.junit=junit-unit.xml

      - name: Install Playwright browsers (Chromium only for CI)
        working-directory: apps/site
        run: npx playwright install --with-deps chromium

      - name: Run E2E tests with dynamic ports
        working-directory: apps/site
        run: |
          # Debug: Show allocated ports
          echo "ðŸ” Debugging port allocation:"
          echo "DEV_SERVER_PORT: ${DEV_SERVER_PORT:-not set}"
          echo "PREVIEW_SERVER_PORT: ${PREVIEW_SERVER_PORT:-not set}"
          echo "TEST_SERVER_PORT: ${TEST_SERVER_PORT:-not set}"
          
          # Use environment variable or default port
          
          # Use allocated port or default
          ACTUAL_PORT=${DEV_SERVER_PORT:-4321}
          echo "ðŸš€ Starting dev server on port: ${ACTUAL_PORT}"
          
          # Start dev server in background with allocated port
          PORT=${ACTUAL_PORT} npm run dev &
          DEV_PID=$!
          
          # Wait for server with timeout
          timeout 60 bash -c "until curl -s http://localhost:${ACTUAL_PORT} > /dev/null; do sleep 2; done" || {
            echo "âŒ Dev server failed to start on port ${ACTUAL_PORT}"
            echo "ðŸ“Š Port status check:"
            ss -tlnp | grep ":${ACTUAL_PORT}" || echo "No process found on port ${ACTUAL_PORT}"
            kill $DEV_PID 2>/dev/null || true
            exit 1
          }
          
          echo "âœ… Dev server started successfully on port ${ACTUAL_PORT}"
          
          # Run E2E tests with correct base URL
          BASE_URL=http://localhost:${ACTUAL_PORT} npm run test:e2e || E2E_RESULT=$?
          
          # Stop dev server
          kill $DEV_PID 2>/dev/null || true
          
          # Exit with E2E test result
          exit ${E2E_RESULT:-0}

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-test-results-${{ matrix.node-version }}-${{ github.run_id }}
          path: |
            apps/site/coverage/
            apps/site/playwright-report/
            apps/site/junit-unit.xml
          retention-days: 7

  # Job 6: Security Scanning (Optimized)
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    timeout-minutes: 8  # Reduced from 10
    needs: quality-checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run Trivy vulnerability scanner (filesystem only)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          timeout: '5m'

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run npm audit
        run: cd apps/site && npm audit --audit-level=high

      - name: Check for Python security vulnerabilities
        run: |
          python -m pip install --upgrade pip
          pip install safety bandit[toml]
          cd pipelines
          timeout 180 safety check --json > safety-report.json || echo "Safety check completed with warnings"
          timeout 180 bandit -r . -f json -o bandit-report.json -ll || echo "Bandit scan completed with warnings"
        continue-on-error: true

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports-${{ github.run_id }}
          path: |
            pipelines/safety-report.json
            pipelines/bandit-report.json
            trivy-results.sarif
          retention-days: 30

  # Job 7: Quality Gate Summary (Enhanced)
  quality-gate:
    name: Quality Gate Summary
    runs-on: ubuntu-latest
    needs: [quality-checks, build-validation, content-validation, pipeline-tests, frontend-tests, security-scan]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all analysis artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true

      - name: Generate comprehensive quality gate summary
        run: |
          chmod +x scripts/generate-analysis-summary.py
          python scripts/generate-analysis-summary.py \
            --workspace . \
            --output final-quality-gate-summary.md \
            --format markdown \
            --verbose

      - name: Check all quality gates
        id: quality-check
        run: |
          echo "Quality Gate Results:"
          echo "===================="
          echo "Code Quality: ${{ needs.quality-checks.result }}"
          echo "Build Validation: ${{ needs.build-validation.result }}"
          echo "Content Validation: ${{ needs.content-validation.result }}"
          echo "Pipeline Tests: ${{ needs.pipeline-tests.result }}"
          echo "Frontend Tests: ${{ needs.frontend-tests.result }}"
          echo "Security Scan: ${{ needs.security-scan.result }}"
          
          # Determine overall status
          CRITICAL_FAILED=false
          
          # Check critical jobs (must pass)
          if [[ "${{ needs.quality-checks.result }}" == "failure" || \
                "${{ needs.build-validation.result }}" == "failure" || \
                "${{ needs.pipeline-tests.result }}" == "failure" ]]; then
            CRITICAL_FAILED=true
          fi
          
          # Check important jobs (warnings allowed)
          WARNINGS=false
          if [[ "${{ needs.content-validation.result }}" != "success" || \
                "${{ needs.frontend-tests.result }}" != "success" || \
                "${{ needs.security-scan.result }}" != "success" ]]; then
            WARNINGS=true
          fi
          
          # Set outputs
          echo "critical-failed=$CRITICAL_FAILED" >> $GITHUB_OUTPUT
          echo "has-warnings=$WARNINGS" >> $GITHUB_OUTPUT
          
          if [[ "$CRITICAL_FAILED" == "true" ]]; then
            echo "âŒ Quality gate FAILED - Critical jobs failed"
            exit 1
          elif [[ "$WARNINGS" == "true" ]]; then
            echo "âš ï¸ Quality gate PASSED with warnings"
            exit 0
          else
            echo "âœ… Quality gate PASSED - All checks successful"
            exit 0
          fi

      - name: Upload final quality gate summary
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: quality-gate-summary-${{ github.run_id }}
          path: final-quality-gate-summary.md
          retention-days: 30

      - name: Comment PR with comprehensive results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            
            // Build results summary
            const results = {
              'Code Quality': '${{ needs.quality-checks.result }}',
              'Build Validation': '${{ needs.build-validation.result }}',
              'Content Validation': '${{ needs.content-validation.result }}',
              'Pipeline Tests': '${{ needs.pipeline-tests.result }}',
              'Frontend Tests': '${{ needs.frontend-tests.result }}',
              'Security Scan': '${{ needs.security-scan.result }}'
            };
            
            let comment = '## ðŸš¦ Quality Gate Results\n\n';
            
            // Add status summary
            const criticalFailed = '${{ steps.quality-check.outputs.critical-failed }}' === 'true';
            const hasWarnings = '${{ steps.quality-check.outputs.has-warnings }}' === 'true';
            
            if (criticalFailed) {
              comment += 'ðŸš« **QUALITY GATE FAILED** - Critical issues must be resolved\n\n';
            } else if (hasWarnings) {
              comment += 'âš ï¸ **QUALITY GATE PASSED WITH WARNINGS** - Review recommended\n\n';
            } else {
              comment += 'ðŸŽ‰ **QUALITY GATE PASSED** - All checks successful!\n\n';
            }
            
            // Add detailed results
            comment += '### Detailed Results\n\n';
            for (const [job, result] of Object.entries(results)) {
              const icon = result === 'success' ? 'âœ…' : 
                          result === 'failure' ? 'âŒ' : 
                          result === 'cancelled' ? 'â¹ï¸' : 'âš ï¸';
              comment += `${icon} **${job}**: ${result}\n`;
            }
            
            // Add analysis summary if available
            try {
              const analysisContent = fs.readFileSync('final-quality-gate-summary.md', 'utf8');
              comment += '\n---\n\n';
              comment += '### ðŸ“Š Detailed Analysis\n\n';
              comment += analysisContent.substring(0, 1000) + (analysisContent.length > 1000 ? '...\n\n[View full report in artifacts]' : '');
            } catch (e) {
              // Analysis summary not available
            }
            
            comment += '\n---\n';
            comment += `**Build ID:** \`${{ github.run_id }}\`\n`;
            comment += `**Commit:** \`${{ github.sha }}\`\n`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });